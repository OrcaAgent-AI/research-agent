"""Research Agent Graph Module.

This module defines the LangGraph-based research agent that performs
multi-step web research with reflection and citation generation.
"""

import logging
import os

from dotenv import load_dotenv
from langchain.chat_models import init_chat_model
from langchain_core.messages import AIMessage
from langchain_core.runnables import RunnableConfig
from langgraph.graph import END, START, StateGraph
from langgraph.types import Send

from agent.context import Context
from agent.prompts import (
    answer_instructions,
    get_current_date,
    query_writer_instructions,
    reflection_instructions,
    web_searcher_instructions,
)
from agent.state import (
    OverallState,
    QueryGenerationState,
    ReflectionState,
    WebSearchState,
)
from agent.tools_and_schemas import Reflection, SearchQueryList
from agent.utils import (
    get_research_topic,
)

load_dotenv()

# é…ç½®æ—¥å¿—è®°å½•å™¨
logger = logging.getLogger(__name__)

# ç¡®ä¿ LangSmith è¿½è¸ªè¢«å¯ç”¨ï¼ˆå¯é€‰ï¼‰
os.environ.setdefault("LANGCHAIN_TRACING_V2", "true")


def generate_query(state: OverallState, config: RunnableConfig) -> QueryGenerationState:
    """LangGraph node that generates search queries based on the User's question.

    Uses Gemini 2.0 Flash to create an optimized search queries for web research based on
    the User's question.

    Args:
        state: Current graph state containing the User's question
        config: Context for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated queries
    """
    configurable = Context.from_runnable_config(config)

    # check for custom initial search query count
    if state.get("initial_search_query_count") is None:
        state["initial_search_query_count"] = configurable.number_of_initial_queries
    llm = init_chat_model(
        model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
        model_provider=os.getenv("MODEL_PROVIDER", "openai"),
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        temperature=1.0,
        max_retries=2,
    )
    structured_llm = llm.with_structured_output(SearchQueryList)

    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = query_writer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        number_queries=state["initial_search_query_count"],
    )
    # Generate the search queries
    result = structured_llm.invoke(formatted_prompt)
    return {"search_query": result.query}


def continue_to_web_research(state: QueryGenerationState):
    """LangGraph node that sends the search queries to the web research node.

    This is used to spawn n number of web research nodes, one for each search query.
    """
    return [
        Send("web_research", {"search_query": search_query, "id": int(idx)})
        for idx, search_query in enumerate(state["search_query"])
    ]


from langchain_community.tools.tavily_search import TavilySearchResults


def web_research(state: WebSearchState, config: RunnableConfig) -> OverallState:
    """LangGraph node that performs web research using Tavily Search API + LLM synthesis.

    Executes a web search using Tavily, then uses an LLM to synthesize the results
    into a well-structured, cited summary following the web_searcher_instructions.

    Args:
        state: Current graph state containing the search query
        config: Context for the runnable

    Returns:
        Dictionary with state update, including sources and research results
    """
    # åˆå§‹åŒ– Tavily æœç´¢
    tavily_search = TavilySearchResults(
        max_results=5,
        search_depth="advanced",
        include_answer=True,
    )

    # æ‰§è¡Œæœç´¢
    search_query = state["search_query"]

    try:
        logger.info(f"ğŸ” Starting Tavily search for query: {search_query}")
        search_results = tavily_search.invoke({"query": search_query})
        logger.info("âœ… Tavily search completed")

    except Exception as e:
        error_msg = str(e)
        logger.error(f"âŒ Tavily search exception: {error_msg}")

        # æ£€æµ‹ç‰¹å®šé”™è¯¯ç±»å‹
        if "432" in error_msg or "Client Error" in error_msg:
            logger.error("ğŸš« Tavily API Error 432: API é…é¢å·²ç”¨å®Œæˆ– API Key æ— æ•ˆ")
            logger.error("ğŸ’¡ è§£å†³æ–¹æ¡ˆï¼š")
            logger.error("   1. æ£€æŸ¥ .env æ–‡ä»¶ä¸­çš„ TAVILY_API_KEY")
            logger.error("   2. è®¿é—® https://tavily.com æ£€æŸ¥é…é¢")
            logger.error("   3. å¦‚éœ€è¦ï¼Œç”³è¯·æ–°çš„ API Key")
        elif "401" in error_msg or "Unauthorized" in error_msg:
            logger.error("ğŸš« Tavily API Error 401: API Key æ— æ•ˆæˆ–æœªæˆæƒ")
        elif "429" in error_msg or "Too Many Requests" in error_msg:
            logger.error("ğŸš« Tavily API Error 429: è¯·æ±‚è¿‡äºé¢‘ç¹ï¼Œè§¦å‘é™æµ")
        elif "timeout" in error_msg.lower():
            logger.error("ğŸš« Tavily API Timeout: è¯·æ±‚è¶…æ—¶")
        else:
            logger.error(f"ğŸš« Tavily API æœªçŸ¥é”™è¯¯: {error_msg}")

        search_results = []

    # è°ƒè¯•ï¼šæ‰“å°æœç´¢ç»“æœç±»å‹å’Œå†…å®¹
    logger.info(f"ğŸ“Š Search results type: {type(search_results)}")

    # æ£€æŸ¥æ˜¯å¦è¿”å›äº†é”™è¯¯å¯¹è±¡ï¼ˆå­—ç¬¦ä¸²å½¢å¼çš„é”™è¯¯ï¼‰
    if isinstance(search_results, str):
        logger.error(f"âŒ Tavily returned error string: {search_results}")
        if "HTTPError" in search_results or "432" in search_results:
            logger.error("ğŸš« Tavily API é…é¢é”™è¯¯ (HTTP 432)")
            logger.error("ğŸ’¡ è¯·æ£€æŸ¥æ‚¨çš„ Tavily API é…é¢å’Œ Key æœ‰æ•ˆæ€§")
        search_results = []

    # ç¡®ä¿ search_results æ˜¯åˆ—è¡¨
    if not isinstance(search_results, list):
        logger.warning(f"âš ï¸ Unexpected search_results type: {type(search_results)}")
        logger.warning("âš ï¸ Converting to empty list")
        search_results = []

    logger.info(f"ğŸ“Š Search results count: {len(search_results)}")

    # å¤„ç†ç»“æœ
    sources_gathered = []
    raw_results_for_llm = []  # ç”¨äºä¼ é€’ç»™ LLM çš„åŸå§‹ç»“æœ

    if len(search_results) == 0:
        logger.error(f"âŒ No search results returned for query: '{search_query}'")
        logger.warning("âš ï¸ è¿”å›å ä½ç¬¦ä»¥é¿å…æµç¨‹ä¸­æ–­")
        # è¿”å›ä¸€ä¸ªå ä½ç¬¦ï¼Œé¿å…å®Œå…¨å¤±è´¥
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": [
                f"âš ï¸ æœªèƒ½è·å–å…³äº '{search_query}' çš„æœç´¢ç»“æœï¼ˆå¯èƒ½æ˜¯ API é…é¢é™åˆ¶ï¼‰ã€‚"
            ],
        }

    # æ­¥éª¤1: æ”¶é›†æ¥æºä¿¡æ¯å’ŒåŸå§‹å†…å®¹
    for idx, result in enumerate(search_results):
        citation_id = f"[{state['id']}-{idx}]"

        # æ£€æŸ¥ result æ˜¯å¦ä¸ºå­—å…¸
        if not isinstance(result, dict):
            logger.warning(f"âš ï¸ Skipping non-dict result at index {idx}: {type(result)}")
            continue

        logger.info(
            f"ğŸ“„ Processing result {idx}: url={result.get('url', 'N/A')}, title={result.get('title', 'N/A')}"
        )

        # æ”¶é›†æ¥æºä¿¡æ¯
        url = result.get("url", "")
        title = result.get("title", "æœªçŸ¥æ ‡é¢˜")
        content = result.get("content", "")

        if not url:
            logger.warning(f"âš ï¸ Result {idx} has no URL, skipping")
            continue

        sources_gathered.append({"url": url, "title": title, "citation_id": citation_id})

        # å‡†å¤‡ç»™ LLM çš„ç»“æ„åŒ–æ•°æ®
        if content:
            raw_results_for_llm.append(
                {
                    "citation_id": citation_id,
                    "title": title,
                    "url": url,
                    "content": content[:2000],  # é™åˆ¶æ¯ä¸ªç»“æœçš„é•¿åº¦ï¼Œé¿å… token è¶…é™
                }
            )
            logger.info(f"âœ… Prepared content for LLM synthesis with citation_id: {citation_id}")
        else:
            logger.warning(f"âš ï¸ Result {idx} has no content")

    logger.info(f"ğŸ“Š Collected {len(sources_gathered)} sources for query: {search_query}")

    if len(sources_gathered) == 0:
        logger.error(f"âŒ No sources collected! search_results was: {search_results}")
        return {
            "sources_gathered": [],
            "search_query": [state["search_query"]],
            "web_research_result": ["âš ï¸ æœªèƒ½æ”¶é›†åˆ°æœ‰æ•ˆçš„æœç´¢æ¥æºã€‚"],
        }

    # æ­¥éª¤2: ä½¿ç”¨ LLM åˆæˆé«˜è´¨é‡æ‘˜è¦ï¼ˆå¸¦å¼•ç”¨ï¼‰
    logger.info("ğŸ¤– Using LLM to synthesize search results into structured summary...")

    # æ„å»ºç»™ LLM çš„æœç´¢ç»“æœæ–‡æœ¬
    search_results_text = ""
    for item in raw_results_for_llm:
        search_results_text += f"\n\n--- Source {item['citation_id']} ---\n"
        search_results_text += f"Title: {item['title']}\n"
        search_results_text += f"URL: {item['url']}\n"
        search_results_text += f"Content: {item['content']}\n"

    # æ ¼å¼åŒ– prompt
    current_date = get_current_date()
    formatted_prompt = web_searcher_instructions.format(
        current_date=current_date,
        research_topic=search_query,
    )

    # æ·»åŠ æœç´¢ç»“æœåˆ° prompt
    formatted_prompt += f"\n\nSearch Results:\n{search_results_text}"
    formatted_prompt += "\n\nIMPORTANT INSTRUCTIONS:"
    formatted_prompt += (
        "\n- You MUST include citation markers (e.g., [0-0], [0-1]) at the END of each sentence"
    )
    formatted_prompt += "\n- Use the exact citation_id format provided in the sources above"
    formatted_prompt += "\n- Example: 'QEMU is an open-source virtualization tool [0-0]. It supports multiple architectures [0-1].'"
    formatted_prompt += (
        "\n- Write a well-structured summary (200-400 words) that synthesizes the key findings"
    )
    formatted_prompt += "\n- Focus on factual information from the search results only"

    # åˆå§‹åŒ– LLM
    llm = init_chat_model(
        model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
        model_provider=os.getenv("MODEL_PROVIDER", "openai"),
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        temperature=0.3,  # è¾ƒä½æ¸©åº¦ç¡®ä¿æ›´å‡†ç¡®çš„å¼•ç”¨
        max_retries=2,
    )

    try:
        # è°ƒç”¨ LLM ç”Ÿæˆæ‘˜è¦
        synthesized_result = llm.invoke(formatted_prompt)
        synthesized_text = synthesized_result.content

        logger.info(f"âœ… LLM synthesis completed, length: {len(synthesized_text)} chars")
        logger.info(f"ğŸ“ Synthesized text preview: {synthesized_text[:200]}...")

    except Exception as e:
        logger.error(f"âŒ LLM synthesis failed: {e!s}")
        logger.warning("âš ï¸ Falling back to simple concatenation")

        # å›é€€æ–¹æ¡ˆï¼šç®€å•æ‹¼æ¥
        research_text_parts = []
        for item in raw_results_for_llm:
            research_text_parts.append(f"{item['content']} {item['citation_id']}")
        synthesized_text = "\n\n".join(research_text_parts)

    logger.info(f"âœ… Successfully collected {len(sources_gathered)} sources")
    logger.info(f"ğŸ”— First source: {sources_gathered[0] if sources_gathered else 'N/A'}")

    return {
        "sources_gathered": sources_gathered,
        "search_query": [state["search_query"]],
        "web_research_result": [synthesized_text],
    }


def reflection(state: OverallState, config: RunnableConfig) -> ReflectionState:
    """LangGraph node that identifies knowledge gaps and generates potential follow-up queries.

    Analyzes the current summary to identify areas for further research and generates
    potential follow-up queries. Uses structured output to extract
    the follow-up query in JSON format.

    Args:
        state: Current graph state containing the running summary and research topic
        config: Context for the runnable, including LLM provider settings

    Returns:
        Dictionary with state update, including search_query key containing the generated follow-up query
    """
    configurable = Context.from_runnable_config(config)
    # Increment the research loop count and get the reasoning model
    current_loop_count = state.get("research_loop_count", 0) + 1

    logger.info(f"ğŸ”„ Reflection Loop {current_loop_count} starting...")
    logger.info(f"ğŸ“Š Current search query count: {len(state.get('search_query', []))}")
    logger.info(f"ğŸ“š Web research results count: {len(state.get('web_research_result', []))}")

    # Format the prompt
    current_date = get_current_date()

    # æˆªæ–­æ‘˜è¦ä»¥é˜²æ­¢ token è¶…é™
    # æ¯ä¸ªæ‘˜è¦æœ€å¤šä¿ç•™å‰ 1000 ä¸ªå­—ç¬¦
    web_results = state.get("web_research_result", [])
    truncated_summaries = []
    for idx, summary in enumerate(web_results):
        truncated = summary[:1000] if len(summary) > 1000 else summary
        if len(summary) > 1000:
            truncated += f"\n... [æ‘˜è¦ {idx + 1} å·²æˆªæ–­ï¼ŒåŸé•¿åº¦: {len(summary)} å­—ç¬¦]"
        truncated_summaries.append(truncated)

    formatted_prompt = reflection_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n\n---\n\n".join(truncated_summaries),
    )

    # init Reasoning Model with increased max_tokens
    llm = init_chat_model(
        model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
        model_provider=os.getenv("MODEL_PROVIDER", "openai"),
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        temperature=1.0,
        max_retries=2,
        max_tokens=2000,
    )
    result = llm.with_structured_output(Reflection).invoke(formatted_prompt)

    logger.info(
        f"âœ… Reflection result: is_sufficient={result.is_sufficient}, follow_up_queries={len(result.follow_up_queries)}"
    )

    return {
        "is_sufficient": result.is_sufficient,
        "knowledge_gap": result.knowledge_gap,
        "follow_up_queries": result.follow_up_queries,
        "research_loop_count": current_loop_count,
        "number_of_ran_queries": len(state.get("search_query", [])),
    }


def evaluate_research(
    state: ReflectionState,
    config: RunnableConfig,
) -> OverallState:
    """LangGraph routing function that determines the next step in the research flow.

    Controls the research loop by deciding whether to continue gathering information
    or to finalize the summary based on the configured maximum number of research loops.

    Args:
        state: Current graph state containing the research loop count
        config: Context for the runnable, including max_research_loops setting

    Returns:
        String literal indicating the next node to visit ("web_research" or "finalize_answer")
    """
    configurable = Context.from_runnable_config(config)
    max_research_loops = (
        state.get("max_research_loops")
        if state.get("max_research_loops") is not None
        else configurable.max_research_loops
    )

    current_loop = state.get("research_loop_count", 0)
    is_sufficient = state.get("is_sufficient", False)
    follow_up_count = len(state.get("follow_up_queries", []))

    logger.info(f"ğŸ” Evaluate Research - Loop: {current_loop}/{max_research_loops}")
    logger.info(f"ğŸ“Š Is Sufficient: {is_sufficient}")
    logger.info(f"â“ Follow-up queries: {follow_up_count}")

    # æ£€æŸ¥ç»ˆæ­¢æ¡ä»¶
    if is_sufficient:
        logger.info("âœ… Research is sufficient, finalizing answer...")
        return "finalize_answer"
    elif current_loop >= max_research_loops:
        logger.info(f"âš ï¸ Max research loops ({max_research_loops}) reached, finalizing answer...")
        return "finalize_answer"
    else:
        logger.info(f"ğŸ”„ Continuing research with {follow_up_count} follow-up queries...")
        return [
            Send(
                "web_research",
                {
                    "search_query": follow_up_query,
                    "id": state["number_of_ran_queries"] + int(idx),
                },
            )
            for idx, follow_up_query in enumerate(state["follow_up_queries"])
        ]


def finalize_answer(state: OverallState, config: RunnableConfig):
    """LangGraph node that finalizes the research summary.

    Prepares the final output by deduplicating and formatting sources, then
    combining them with the running summary to create a well-structured
    research report with proper citations.

    Args:
        state: Current graph state containing the running summary and sources gathered

    Returns:
        Dictionary with state update, including running_summary key containing the formatted final summary with sources
    """
    # Format the prompt
    current_date = get_current_date()
    formatted_prompt = answer_instructions.format(
        current_date=current_date,
        research_topic=get_research_topic(state["messages"]),
        summaries="\n---\n\n".join(state["web_research_result"]),
    )

    # init Reasoning Model
    llm = init_chat_model(
        model=os.getenv("MODEL_NAME", "gpt-4o-mini"),
        model_provider=os.getenv("MODEL_PROVIDER", "openai"),
        api_key=os.getenv("OPENAI_API_KEY"),
        base_url=os.getenv("OPENAI_BASE_URL"),
        temperature=1.0,
        max_retries=2,
        max_tokens=2000,
    )
    result = llm.invoke(formatted_prompt)

    # ============ å­¦æœ¯é£æ ¼å¼•ç”¨ç³»ç»Ÿ ============
    content = result.content

    # æ­¥éª¤0: ç§»é™¤é”™è¯¯çš„ä»£ç å—æ ‡è®°ï¼ˆå¦‚æœLLMè¯¯ç”Ÿæˆäº†bash/codeå—ï¼‰
    import re

    content = re.sub(r"```[\w]*\n", "", content)  # ç§»é™¤å¼€å§‹æ ‡è®°
    content = re.sub(r"\n```", "", content)  # ç§»é™¤ç»“æŸæ ‡è®°
    content = content.replace("```", "")  # ç§»é™¤ä»»ä½•æ®‹ç•™çš„```

    logger.info(f"ğŸ“ Original content preview: {content[:500]}...")

    # æ­¥éª¤1: æ„å»º citation_id -> source çš„æ˜ å°„
    citation_map = {}
    all_sources = state.get("sources_gathered", [])
    logger.info(f"ğŸ“Š Total sources_gathered from state: {len(all_sources)}")

    # æ£€æŸ¥æ˜¯å¦æœ‰æ¥æº
    if len(all_sources) == 0:
        logger.error("âŒ CRITICAL: No sources_gathered in state!")
        logger.error("ğŸ’¡ å¯èƒ½çš„åŸå› :")
        logger.error("   1. Tavily API é…é¢å·²ç”¨å®Œ (HTTP 432)")
        logger.error("   2. æ‰€æœ‰æœç´¢æŸ¥è¯¢éƒ½å¤±è´¥äº†")
        logger.error("   3. ç½‘ç»œè¿æ¥é—®é¢˜")
        logger.warning("âš ï¸ å°†ç”Ÿæˆä¸å¸¦å¼•ç”¨çš„ç­”æ¡ˆ")

    for source in all_sources:
        citation_id = source.get("citation_id", "")
        if citation_id:
            citation_map[citation_id] = source

    logger.info(f"ğŸ“š Total available sources in citation_map: {len(citation_map)}")
    if citation_map:
        logger.info(
            f"ğŸ” Citation map keys sample: {list(citation_map.keys())[:10]}"
        )  # åªæ˜¾ç¤ºå‰10ä¸ª

    # æ­¥éª¤2: æå–å®é™…ä½¿ç”¨çš„å¼•ç”¨ï¼ˆæŒ‰å‡ºç°é¡ºåºï¼‰
    used_sources = []  # æœ‰åºåˆ—è¡¨
    citation_to_number = {}  # citation_id -> å¼•ç”¨ç¼–å·ï¼ˆå¦‚ 1, 2, 3ï¼‰
    seen_urls = set()

    # éå†æ‰€æœ‰å¯èƒ½çš„ citation_idï¼Œæ£€æŸ¥æ˜¯å¦åœ¨æ–‡æœ¬ä¸­å‡ºç°
    for citation_id, source in citation_map.items():
        if citation_id in content:
            logger.info(f"âœ… Found citation_id in content: {citation_id}")
            url = source.get("url", "")

            # å»é‡ï¼šç›¸åŒ URL åªä¿ç•™ä¸€ä¸ªç¼–å·
            if url and url in seen_urls:
                # æŸ¥æ‰¾å·²æœ‰çš„å¼•ç”¨ç¼–å·
                for idx, existing_source in enumerate(used_sources, 1):
                    if existing_source.get("url") == url:
                        citation_to_number[citation_id] = idx
                        break
            else:
                # æ–°æ¥æºï¼Œåˆ†é…æ–°ç¼–å·
                used_sources.append(source)
                citation_to_number[citation_id] = len(used_sources)
                if url:
                    seen_urls.add(url)
        else:
            logger.debug(f"âŒ Citation_id NOT found in content: {citation_id}")

    logger.info(f"âœ… Used sources in final answer: {len(used_sources)}")
    logger.info(f"ğŸ“ Citation mapping: {citation_to_number}")

    # æ­¥éª¤3: å¦‚æœæ²¡æœ‰æ‰¾åˆ°ä»»ä½•å¼•ç”¨ï¼Œä½¿ç”¨æ‰€æœ‰æ¥æº
    if not used_sources and all_sources:
        logger.warning("âš ï¸ No citations found in LLM output, using all sources")
        used_sources = all_sources

    # æ­¥éª¤4: æ›¿æ¢ citation_id ä¸ºæ ‡å‡†å­¦æœ¯å¼•ç”¨æ ¼å¼ [æ•°å­—]
    content_with_citations = content
    for citation_id, ref_number in sorted(
        citation_to_number.items(), key=lambda x: len(x[0]), reverse=True
    ):
        if citation_id in content_with_citations:
            inline_citation = f"[{ref_number}]"
            logger.info(f"ğŸ”— Replacing '{citation_id}' with '{inline_citation}'")
            content_with_citations = content_with_citations.replace(citation_id, inline_citation)

    # æ­¥éª¤5: ä¼˜åŒ–å¼•ç”¨ä½ç½®ï¼ˆç§»é™¤å¤šä½™ç©ºæ ¼å’Œæ¢è¡Œï¼‰
    content_with_citations = re.sub(
        r"\n+\s*(\[\d+\])",  # å¤šä¸ªæ¢è¡Œ + å¯èƒ½çš„ç©ºæ ¼ + [1]
        r" \1",  # å•ä¸ªç©ºæ ¼ + [1]
        content_with_citations,
    )

    content_with_citations = re.sub(
        r"(\[\d+\])\s*\n(?!\n)",  # [1] + ç©ºæ ¼ + å•æ¢è¡Œï¼ˆåé¢ä¸æ˜¯æ¢è¡Œï¼‰
        r"\1 ",  # [1] + ç©ºæ ¼
        content_with_citations,
    )

    content_with_citations = re.sub(
        r"(\[\d+\])\s+(\[\d+\])",  # [1]  [2]
        r"\1 \2",  # [1] [2]
        content_with_citations,
    )

    # æ­¥éª¤6: æ¸…ç†LLMå¯èƒ½ç”Ÿæˆçš„å‚è€ƒæ–‡çŒ®éƒ¨åˆ†ï¼Œé¿å…é‡å¤
    content_with_citations = re.sub(
        r"\n*#+\s*å‚è€ƒæ–‡çŒ®.*", "", content_with_citations, flags=re.IGNORECASE | re.DOTALL
    )
    content_with_citations = re.sub(
        r"\n*å‚è€ƒæ–‡çŒ®[:ï¼š].*", "", content_with_citations, flags=re.IGNORECASE | re.DOTALL
    )
    content_with_citations = re.sub(
        r"\n*References[:ï¼š].*", "", content_with_citations, flags=re.IGNORECASE | re.DOTALL
    )

    # æ­¥éª¤7: åœ¨æ–‡ç« åº•éƒ¨æ·»åŠ ç»Ÿä¸€çš„å‚è€ƒæ–‡çŒ®åˆ—è¡¨
    if used_sources:
        logger.info(f"ğŸ“š Preparing to add {len(used_sources)} references to the final answer")
        references = "\n\n---\n\n## ğŸ“š å‚è€ƒæ–‡çŒ®\n\n"
        for idx, source in enumerate(used_sources, 1):
            url = source.get("url", "")
            title = source.get("title", "Untitled")

            # å­¦æœ¯å¼•ç”¨æ ¼å¼ï¼š[ç¼–å·] æ ‡é¢˜ - URL
            references += f"[{idx}] {title}\n"
            if url:
                references += f"    {url}\n\n"
            else:
                references += "    (URLæœªæä¾›)\n\n"

        content_with_citations += references
        logger.info(f"âœ… Successfully added {len(used_sources)} references to the final answer")
    else:
        logger.error("âŒ ERROR: No sources available! Check sources_gathered in state!")
        logger.error("ğŸ” Diagnostic Information:")
        logger.error(f"   - Total sources_gathered: {len(all_sources)}")
        logger.error(f"   - Citation map size: {len(citation_map)}")
        logger.error(f"   - Used sources: {len(used_sources)}")
        logger.error("ğŸ’¡ This usually happens when:")
        logger.error("   1. Tavily API quota is exhausted (HTTP 432)")
        logger.error("   2. All web searches failed")
        logger.error("   3. LLM didn't preserve citation markers from summaries")

        # æ·»åŠ è­¦å‘Šè¯´æ˜è€Œä¸æ˜¯ç©ºåˆ—è¡¨
        content_with_citations += "\n\n---\n\n## ğŸ“š å‚è€ƒæ–‡çŒ®\n\n"
        content_with_citations += (
            "*âš ï¸ ç”±äº API é™åˆ¶ï¼Œæ— æ³•æä¾›å‚è€ƒæ–‡çŒ®æ¥æºã€‚å»ºè®®æ£€æŸ¥ Tavily API é…é¢ã€‚*\n"
        )

    return {
        "messages": [AIMessage(content=content_with_citations)],
        "sources_gathered": used_sources,
    }


# Create the research agent graph
builder = StateGraph(OverallState, config_schema=Context)

# Define the nodes
builder.add_node("generate_query", generate_query)
builder.add_node("web_research", web_research)
builder.add_node("reflection", reflection)
builder.add_node("finalize_answer", finalize_answer)

# Set the entry point
builder.add_edge(START, "generate_query")

# Add conditional edge to continue with search queries in a parallel branch
builder.add_conditional_edges("generate_query", continue_to_web_research, ["web_research"])

# Reflect on the web research
builder.add_edge("web_research", "reflection")

# Evaluate the research
builder.add_conditional_edges("reflection", evaluate_research, ["web_research", "finalize_answer"])

# Finalize the answer
builder.add_edge("finalize_answer", END)

# Compile the graph
graph = builder.compile(
    name="pro-search-agent",
    # Explicitly declare this is a chat-compatible graph
    interrupt_before=None,
    interrupt_after=None,
)
